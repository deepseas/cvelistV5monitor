<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>CVE Feed for llama.cpp -- llama.cpp</title>
    <link>https://raw.githubusercontent.com/deepseas/cvelistV5monitor/main/feeds/llama.cpp/llama.cpp.rss</link>
    <description>The latest CVEs for llama.cpp -- llama.cpp</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Tue, 04 Jun 2024 17:29:25 +0000</lastBuildDate>
    <ttl>60</ttl>
    <item>
      <title>CVE-2024-21836|2024-02-26T18:00:07.188Z -- llama.cpp -- llama.cpp
</title>
      <link>https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-21836</link>
      <description>A heap-based buffer overflow vulnerability exists in the GGUF library header.n_tensors functionality of llama.cpp Commit 18c2e17. A specially crafted .gguf file can lead to code execution. An attacker can provide a malicious file to trigger this vulnerability.</description>
      <guid isPermaLink="false">CVE-2024-21836|2024-02-26T18:00:07.188Z</guid>
      <pubDate>Mon, 26 Feb 2024 16:07:51 +0000</pubDate>
    </item>
  </channel>
</rss>
