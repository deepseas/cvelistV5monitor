<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>CVE Feed for ggerganov -- all</title>
    <link>https://raw.githubusercontent.com/deepseas/cvelistV5monitor/main/feeds/ggerganov/all.rss</link>
    <description>The latest CVEs for ggerganov -- all products</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Mon, 22 Jul 2024 18:24:09 +0000</lastBuildDate>
    <ttl>60</ttl>
    <item>
      <title>CVE-2024-41130|2024-07-22T17:51:55.310Z -- ggerganov -- llama.cpp
</title>
      <link>https://www.cve.org/CVERecord?id=CVE-2024-41130</link>
      <description>llama.cpp provides LLM inference in C/C++. Prior to b3427, llama.cpp contains a null pointer dereference in gguf_init_from_file. This vulnerability is fixed in b3427.</description>
      <guid isPermaLink="false">CVE-2024-41130|2024-07-22T17:51:55.310Z</guid>
      <pubDate>Mon, 22 Jul 2024 17:28:47 +0000</pubDate>
    </item>
    <item>
      <title>CVE-2024-32878|2024-04-26T20:31:53.813Z -- ggerganov -- llama.cpp</title>
      <link>https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2024-32878</link>
      <description>Llama.cpp is LLM inference in C/C++. There is a use of uninitialized heap variable vulnerability in gguf_init_from_file, the code will free this uninitialized variable later. In a simple POC, it will directly cause a crash. If the file is carefully constructed, it may be possible to control this uninitialized value and cause arbitrary address free problems. This may further lead to be exploited. Causes llama.cpp to crash (DoS) and may even lead to arbitrary code execution (RCE). This vulnerability has been patched in commit b2740.</description>
      <guid isPermaLink="false">CVE-2024-32878|2024-04-26T20:31:53.813Z</guid>
      <pubDate>Fri, 26 Apr 2024 20:31:53 +0000</pubDate>
    </item>
    <item>
      <title>CVE-2024-32878|2024-07-03T14:57:34.597Z -- ggerganov -- llama.cpp</title>
      <link>https://www.cve.org/CVERecord?id=CVE-2024-32878</link>
      <description>Llama.cpp is LLM inference in C/C++. There is a use of uninitialized heap variable vulnerability in gguf_init_from_file, the code will free this uninitialized variable later. In a simple POC, it will directly cause a crash. If the file is carefully constructed, it may be possible to control this uninitialized value and cause arbitrary address free problems. This may further lead to be exploited. Causes llama.cpp to crash (DoS) and may even lead to arbitrary code execution (RCE). This vulnerability has been patched in commit b2740.</description>
      <guid isPermaLink="false">CVE-2024-32878|2024-07-03T14:57:34.597Z</guid>
      <pubDate>Fri, 26 Apr 2024 20:31:53 +0000</pubDate>
    </item>
  </channel>
</rss>
